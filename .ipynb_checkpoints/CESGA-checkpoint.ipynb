{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80d4ab7-5538-4a9a-a3a5-f670981cd78b",
   "metadata": {},
   "source": [
    "# 5. Computing at CESGA\n",
    "\n",
    "The users of the infrastructures and services of the CESGA include researchers, developers, technicians, and innovators in public and private institutions.\n",
    "\n",
    "$\\bullet$ Galician Universities\n",
    "\n",
    "$\\bullet$ Regional Research Centres\n",
    "\n",
    "$\\bullet$ The National Scientific Research Council (CSIC)\n",
    "\n",
    "$\\bullet$ Other public or private organisations all over the world, including:\n",
    "\n",
    "- R&D departments of of industries and companies,\n",
    "- technological and research centres oriented to industry,\n",
    "- other Universities all over the world, and\n",
    "- non-profit R&D organisations.\n",
    "\n",
    "CESGA has different computing platforms of different architectures to allow the researcher to always choose the architecture that best suits their calculation needs.\n",
    "\n",
    "For operations that require calculation of high performance and supercomputing, the **FinisTerrae-III supercomputer** offers higher performance and a high performance interconnection network for parallel work or that require the use of GPUs. It also allows operations that require handling large volumes of data. Here are the general characteristics of the FT-III:\n",
    "\n",
    "![alternative text](images/PANEL-FINISTERRAE-ALMACENAMENTO-FINAL_2-600x1364.png)\n",
    "\n",
    "For an individual user, the basic services available are the following:\n",
    "\n",
    "**Queues:**\n",
    "**$HOME:**\n",
    "**$STORE:**\n",
    "**$LUSTRE**\n",
    "\n",
    "\n",
    "\n",
    "## 5.1. Creating an account\n",
    "\n",
    "[See the full process here](images/proceso_usuarios.pdf)\n",
    "\n",
    "It is required to fill the form in this link: https://www.cesga.es/en/community/service-request/\n",
    "\n",
    "and, also, attach a **certified document from your institution supporting your access right.**\n",
    "\n",
    "\n",
    "## 5.2. Connecting to CESGA\n",
    "\n",
    "Once your account is created, you must follow all the suggestions given by email, such as reseting your default password.\n",
    "\n",
    "The first step is just connecting to the remote machine, and this can be done by typing the following on a terminal (it also works in Windows):\n",
    "\n",
    "```\n",
    "ssh -XY YourUser@ft3.cesga.es\n",
    "```\n",
    "\n",
    "In my case the user is **uviirlcc**, just in case of doubts in further commands.\n",
    "\n",
    "After sending that instruction it will ask for your password. Once logged you should see a screen like the folowing:\n",
    "\n",
    "![alternative text](images/cesga_login.png)\n",
    "\n",
    "\n",
    "In order to copy files from local to FT-III (and the other way around), you must type:\n",
    "\n",
    "```scp /path/from/local/to/specific/file YourUser@ft3.cesga.es:/path/in/FTIII``` To copy a regular file from local to remote\n",
    "\n",
    "```scp YourUser@ft3.cesga.es:/path/from/FTIII/to/specific/file path/to/local``` To copy a regular file from remote to local\n",
    "\n",
    "**Important:** ```scp``` is the instruction for copying files, so for nested copies, i.e., to copy **folders** use the ```-r``` flag.\n",
    "\n",
    "## 5.3. Basic commands\n",
    "\n",
    "\n",
    "```sbatch``` Send a script to a SLURM partition. The only mandatory parameters are the estimated time and the estimated memory per node/CPU. \n",
    "\n",
    "For example, to send a script called ```script.sh``` with a duration of 24 hours: ```sbatch -t 24:00:00 --mem=4GB script.sh```\n",
    "\n",
    "If the command is executed successfully, it returns the number of the job (<jobid>).\n",
    "\n",
    "```srun``` Commonly used to run a parallel task on a script controlled by SLURM.\n",
    "\n",
    "```sinfo``` Displays information about SLURM nodes and partitions. It also provides information about:\n",
    "\n",
    "- Existing partitions (PARTITION)\n",
    "\n",
    "- Whether or not they are available (AVAIL)\n",
    "\n",
    "- The maximum time of each partition (TIMELIMIT. If it is infinite then it is regulated externally)\n",
    "\n",
    "- The nodes belonging to each partition (NODES)\n",
    "\n",
    "- Node state, the most common are:\n",
    "```\n",
    "            idle: means available\n",
    "\n",
    "            alloc: means in use\n",
    "\n",
    "            mix: means part of your CPUs are available\n",
    "\n",
    "            resv: means reserved for an specific use\n",
    "\n",
    "            drain: means temporarily removed for technical reasons\n",
    "```\n",
    "\n",
    "- Information about a specific partition: ```sinfo -p <partitionname>```\n",
    "\n",
    "- Information every 60 seconds: ```sinfo -i60```\n",
    "\n",
    "- List reasons nodes are in the down, drained, fail or failing state: ```sinfo -R```\n",
    "\n",
    "```squeue``` Displays information about (onyly your) jobs and their status in the Slurm scheduling queue:\n",
    "\n",
    "- State of a job with the jobid: ```squeue -j <jobid>```\n",
    "\n",
    "- Report the expected start time and resources to be allocated for pending jobs in order of increasing start time: ```squeue --start```\n",
    "\n",
    "- List all the running jobs: ```squeue -t RUNNING```\n",
    "\n",
    "- List all the pending jobs: ```squeue -t PENDING```\n",
    "\n",
    "- List the jobs demanding a specific partition: ```squeue -p <partition name>```\n",
    "\n",
    "You can also see full list of job states here: https://cesga-docs.gitlab.io/ft3-user-guide/batch_jobs_states.html\n",
    "\n",
    "```scancel``` It is used to cancel jobs, job arrays or job steps\n",
    "\n",
    " - Cancel a job: ```scancel <jobid>```\n",
    "\n",
    " - Cancel all pending jobs: ```scancel -t PENDING```\n",
    "\n",
    " - Cancel one or more jobs with name “jobname”: ```scancel --name <jobname>```\n",
    "\n",
    " - Cancel all jobs: ```scancel -u <YourUser>```\n",
    "\n",
    "```scontrol``` Returns detailed information about the nodes, partitions, job steps, and configuration. It is used for monitoring and modifing queued jobs.\n",
    "\n",
    " - Show detailed information about a job: ```scontrol show jobid -dd <jobid>```\n",
    "\n",
    " - Write the batch script for a given job_id to a file or to stdout: ```scontrol write batch_script <jobid> -```\n",
    "\n",
    " - Prevent a pending job from being started (without cancel it): ```scontrol hold <jobid>```\n",
    "\n",
    " - Release a previously held job to begin execution: ```scontrol release <jobid>```\n",
    "\n",
    " - Requeue a running, suspended or finished Slurm batch job into pending state (equivalent to scancel + sbatch): ```scontrol requeue <jobid>```\n",
    "\n",
    "```sqstat``` Detailed information about the queue system, resources consumption, status of all partitions and jobs\n",
    "\n",
    "## 5.4. GPU Nodes\n",
    "\n",
    "https://cesga-docs.gitlab.io/ft3-user-guide/gpu_nodes.html\n",
    "\n",
    "## 5.5. Sending a job to a queue\n",
    "\n",
    "To send a job to a queue it is required a shell script and, as stated before, this shell script must be submitted via ```sbatch``` command. An simple (but efficient) example of how to send a job to **GPU nodes** can be the following:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#----------------------------------------------------\n",
    "# Example SLURM job script to run CUDA applications\n",
    "# on CESGA's FT-III system.\n",
    "#----------------------------------------------------\n",
    "#SBATCH -J test_lunes       # Job name\n",
    "#SBATCH -o test_lunes.o%j   # Name of stdout output file(%j expands to jobId)\n",
    "#SBATCH -e test_lunes.o%j   # Name of stderr output file(%j expands to jobId)\n",
    "#SBATCH -c 32               # Cores per task requested (1 task job). Needed 32 cores per A100 demanded!!!\n",
    "#SBATCH --mem-per-cpu=3G    # memory per core demanded\n",
    "#SBATCH --gres=gpu          # Options for requesting 1GPU\n",
    "#SBATCH -t 01:30:00         # Run time\n",
    "\n",
    "# Run the CUDA application\n",
    "python my_script_that_uses_GPU.py\n",
    "```\n",
    "\n",
    "Remember that the time stamp has the following format: ```days-hours:minutes:seconds```\n",
    "\n",
    "Here is a helpful table with different threshold values for each queue:\n",
    "\n",
    "```\n",
    "Name   Priority       GrpTRES       MaxTRES     MaxWall MaxJobsPU     MaxTRESPU MaxSubmit\n",
    "\n",
    "------------ ---------- ------------- ------------- ----------- --------- ------------- ---\n",
    "\n",
    "short        50                    cpu=2048                    50      cpu=2048       100\n",
    "\n",
    "medium       40                    cpu=2048                    30      cpu=2048        50\n",
    "\n",
    "long         30      cpu=8576      cpu=2048                     5      cpu=2048        10\n",
    "\n",
    "requeue      20                    cpu=2048                     5      cpu=2048        10\n",
    "\n",
    "ondemand     10      cpu=4288      cpu=1024                     2      cpu=1024        10\n",
    "\n",
    "...\n",
    "\n",
    "clk_short    50                      node=1    06:00:00       200       cpu=960       400\n",
    "\n",
    "clk_medium   40                      node=1  3-00:00:00       200       cpu=960       250\n",
    "\n",
    "clk_long     30      cpu=1440        node=1  7-00:00:00        60       cpu=360        60\n",
    "\n",
    "clk_ondemand 10       cpu=720        node=1 42-00:00:00        20       cpu=240        20\n",
    "```\n",
    "\n",
    "## 5.6. Dealing with docker containers in FT-III\n",
    "\n",
    "```\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "# INSTALACIÓN Y EJECUCIÓN DE CONTENEDORES QUE REQUIEREN GPU EN EL FT-III\n",
    "\n",
    "# Los pasos son casi los mismos, pero por temas de las gráficas hay que hacer unas cosas distintas.\n",
    "\n",
    "cd /Directorio/donde/alojar/udocker\n",
    "\n",
    "wget https://github.com/indigo-dc/udocker/releases/download/v1.3.1/udocker-1.3.1.tar.gz\n",
    "\n",
    "tar zxvf udocker-1.3.1.tar.gz\n",
    "\n",
    "export PATH=`pwd`/udocker:$PATH\n",
    "\n",
    "udocker install\n",
    "\n",
    "vi .bashrc\n",
    "\n",
    "*Añadir línea: export PATH=`pwd`/udocker:$PATH\n",
    "\n",
    "[Esc] + wq\n",
    "\n",
    "\n",
    "# Ahora hacemos un pull de la imagen que queremos usar del DockerHub y creamos un contenedor asociado a la misma:\n",
    "\n",
    "udocker pull tensorflow/tensorflow:1.2.0-gpu\n",
    "\n",
    "udocker create --name=contenedor_pointnet [ID de la imagen que queremos vincular]\n",
    "\n",
    "# IMPORTANTE! Ahora debemos pedirle al CESGA que nos dé servicio de GPU, si no crasheará más adelante:\n",
    "\n",
    "copmute --gpu\n",
    "\n",
    "# Esperamos un rato y hacemos:\n",
    "\n",
    "udocker setup --nvidia [ID del contenedor]\n",
    "\n",
    "# Y ahora ya está listo para usarse\n",
    "\n",
    "udocker run --volume=$(pwd) --volume=$LUSTRE(en mi caso) --workdir=$(pwd) [ID del contenedor] /bin/bash\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d6458-bbbc-44a3-8aac-2f0f3584ae47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

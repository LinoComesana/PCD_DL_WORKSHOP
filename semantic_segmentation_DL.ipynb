{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f9d256",
   "metadata": {},
   "source": [
    "# 3. Point cloud segmentation with Deep Learning\n",
    "\n",
    "As we mentioned in the previous section, we are going to classiffy all the points in forest scenes into 2 classes: **trees** and **DTM**. In order to do this, we are going to use a top DL model called [PointNet++](https://github.com/charlesq34/pointnet2).\n",
    "\n",
    "A modified architecture of the model plus all the explained codes are available [within the contents of this workshop](https://github.com/LinoComesana/PCD_DL_WORKSHOP/tree/main/Deep_Learning/pointnet2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bcc74",
   "metadata": {},
   "source": [
    "## 3.1. Generation of training datasets\n",
    "\n",
    "Taking the codes of the previous section as a basis, we can create a script which generates randomly and iteratively different forest point clouds. Here is a quick example that generates a dataset of 10 synthetic point clouds divided into 3 groups: train, test and prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2b984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-c522ce216558>:97: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  m = np.linalg.lstsq(G, z)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "Nube generada. Transcurrido por ahora:  0.7997984965642293  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  1.6077455242474874  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  2.422750965754191  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  3.216011810302734  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  4.012970328330994  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  4.885599788029989  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  5.680546832084656  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  6.481283493836721  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  7.312391149997711  min\n",
      "['environment_workshop.yml', 'udocker_installation', 'data', '.ipynb_checkpoints', 'images', 'overview.ipynb', '.gitignore', 'LECTURAS_segmentacion_cilindrica.py', 'semantic_segmentation_DL.ipynb', 'Deep_Learning', '.git', '__pycache__', 'requirements.ipynb', 'pcd_simulation.ipynb', 'README.md']\n",
      "/home/lino/Documentos/PCD_DL_WORKSHOP/PCD_DL_WORKSHOP/data/trees_pcd\n",
      "The folder already exists, so we pass\n",
      "Nube generada. Transcurrido por ahora:  8.172155408064524  min\n",
      "FIN GENERACIÓN DATASET\n",
      "Duración TOTAL:  8.1721591035525  min\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "import time\n",
    "\n",
    "instante_inicial = time.time()\n",
    "\n",
    "ruta_proyecto_workshop = os.getcwd()\n",
    "\n",
    "print(os.listdir())\n",
    "\n",
    "\n",
    "Number_of_pointclouds = 10\n",
    "\n",
    "for XD in range(Number_of_pointclouds):\n",
    "\n",
    "    eje_x = np.arange(0,100,0.1)\n",
    "    eje_y = np.arange(0,100,0.1)\n",
    "\n",
    "    SUPERFICIE = {}\n",
    "    mesh_x, mesh_y = np.meshgrid(eje_x,eje_y)\n",
    "\n",
    "    xyz = np.zeros((np.size(mesh_x), 3))\n",
    "    xyz[:, 0] = np.reshape(mesh_x, -1)\n",
    "    xyz[:, 1] = np.reshape(mesh_y, -1)\n",
    "\n",
    "\n",
    "    nube_plano = o3d.geometry.PointCloud()\n",
    "    nube_plano.points = o3d.utility.Vector3dVector(xyz)\n",
    "\n",
    "\n",
    "\n",
    "    N_picos = np.random.randint(10,50)\n",
    "    for q in range(N_picos):\n",
    "        indices_puntos = np.arange(0,len(xyz),1)\n",
    "        indice_punto_central_pico = np.random.choice(indices_puntos,size=1)\n",
    "        punto_central = xyz[indice_punto_central_pico]\n",
    "        nube_punto_central = o3d.geometry.PointCloud()\n",
    "        nube_punto_central.points = o3d.utility.Vector3dVector(np.vstack((punto_central,\n",
    "                                                                          punto_central)))\n",
    "        buffer = np.random.randint(10,20)\n",
    "        # Cojo todos los punntos dentro de ese buffer respecto al punto central:\n",
    "        distancias = np.array(nube_plano.compute_point_cloud_distance(nube_punto_central))\n",
    "        indices_dentro_buffer = np.where(distancias <= buffer)[0]\n",
    "\n",
    "        # indices_cambios_altura = indices_puntos\n",
    "        xyz[indices_dentro_buffer,2] = np.random.uniform(buffer/2.,buffer,size=len(indices_dentro_buffer)).T\n",
    "\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # We define some useful functions:\n",
    "    import itertools\n",
    "\n",
    "    Numero_puntos_superficie = 1e6\n",
    "\n",
    "    def exponential_cov(x, y, params):\n",
    "        return params[0] * np.exp( -0.5 * params[1] * np.subtract.outer(x, y)**2)\n",
    "\n",
    "    def conditional(x_new, x, y, params):\n",
    "        B = exponential_cov(x_new, x, params)\n",
    "        C = exponential_cov(x, x, params)\n",
    "        A = exponential_cov(x_new, x_new, params)\n",
    "        mu = np.linalg.inv(C).dot(B.T).T.dot(y)\n",
    "        sigma = A - B.dot(np.linalg.inv(C).dot(B.T))\n",
    "        return(mu.squeeze(), sigma.squeeze())\n",
    "\n",
    "    ordr = 4  # Orden del polinomio al que quiero ajustar en cada \"frame\"\n",
    "\n",
    "    def matriz_minimos_cuadrados(x, y, order=ordr):\n",
    "        \"\"\" generate Matrix use with lstsq \"\"\"\n",
    "        # Genero una matriz con los valores obtenidos por mínimos cuadrados\n",
    "        ncolumnas = (order + 1)**2\n",
    "        G = np.zeros((x.size, ncolumnas))\n",
    "        ij = itertools.product(range(order+1), range(order+1))\n",
    "        for k, (i, j) in enumerate(ij):\n",
    "            G[:, k] = x**i * y**j\n",
    "        return G\n",
    "\n",
    "\n",
    "    puntos = np.copy(xyz)\n",
    "\n",
    "    x, y, z = puntos.T # Hago la traspuesta\n",
    "    x, y = x - x[0], y - y[0]  # Para mejorar la eficacia\n",
    "\n",
    "    # Creamos la matriz que contiene las regresiones por punto:\n",
    "    G = matriz_minimos_cuadrados(x, y, ordr)\n",
    "    # Solve for np.dot(G, m) = z:\n",
    "    # Quiero saber qué valores de m hacen que G·m = z (es decir, np.dot(G,m)=z)\n",
    "    m = np.linalg.lstsq(G, z)[0]\n",
    "\n",
    "\n",
    "    # Evaluamos en una nueva grid el ajuste que acabamos de hacer...\n",
    "    nx, ny = int(np.sqrt(Numero_puntos_superficie)), int(np.sqrt(Numero_puntos_superficie))\n",
    "    xx, yy = np.meshgrid(np.linspace(x.min(), x.max(), nx),\n",
    "                          np.linspace(y.min(), y.max(), ny))\n",
    "\n",
    "\n",
    "    GG = matriz_minimos_cuadrados(xx.ravel(), yy.ravel(), ordr)\n",
    "    zz = np.reshape(np.dot(GG, m), xx.shape)\n",
    "\n",
    "    # Convierto la superficie del fit a una nube de puntos:\n",
    "    superficie_reconstruida = np.zeros((np.size(xx), 3))\n",
    "    superficie_reconstruida[:, 0] = np.reshape(xx, -1)\n",
    "    superficie_reconstruida[:, 1] = np.reshape(yy, -1)\n",
    "    superficie_reconstruida[:, 2] = np.reshape(zz, -1)\n",
    "\n",
    "    pcd2 = o3d.geometry.PointCloud()\n",
    "    pcd2.points = o3d.utility.Vector3dVector(superficie_reconstruida)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Ahora le añadimos algo de ruido gaussiano a cada punto para hacerlo más real:\n",
    "    ruido_x = np.random.normal(0,eje_x[2]-eje_x[1],len(superficie_reconstruida))\n",
    "    ruido_y = np.random.normal(0,eje_y[2]-eje_y[1],len(superficie_reconstruida))\n",
    "    # ruido_z = np.random.normal(0,0.2,len(superficie_reconstruida))\n",
    "\n",
    "    superficie_reconstruida[:, 0] = np.reshape(superficie_reconstruida.take(0,1) + ruido_x, -1)\n",
    "    superficie_reconstruida[:, 1] = np.reshape(superficie_reconstruida.take(1,1) + ruido_y, -1)\n",
    "    # superficie_reconstruida[:, 2] = np.reshape(superficie_reconstruida.take(2,1) + ruido_z, -1)\n",
    "\n",
    "\n",
    "    pcd_DTM = o3d.geometry.PointCloud()\n",
    "    pcd_DTM.points = o3d.utility.Vector3dVector(superficie_reconstruida)\n",
    "\n",
    "    lista_colores_suelo = [np.array([51/255,51/255,0/255]),\n",
    "                     np.array([102/255,102/255,0/255]),\n",
    "                     np.array([153/255,153/255,0/255]),\n",
    "                     np.array([204/255,204/255,0/255]),\n",
    "                     np.array([255/255,255/255,0/255]),\n",
    "                     np.array([102/255,51/255,0/255]),\n",
    "                     np.array([153/255,76/255,0/255]),\n",
    "                     np.array([204/255,102/255,0/255])]\n",
    "\n",
    "    colores_suelo = np.zeros((len(superficie_reconstruida),3))\n",
    "    for e in range(len(colores_suelo)):\n",
    "        colores_suelo[e] = lista_colores_suelo[np.random.choice(len(lista_colores_suelo))]\n",
    "\n",
    "    pcd_DTM.colors = o3d.utility.Vector3dVector(colores_suelo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from LECTURAS_segmentacion_cilindrica import lectura_segmentaciones\n",
    "    import os\n",
    "    import math\n",
    "\n",
    "    print(os.listdir())\n",
    "\n",
    "    os.chdir(ruta_proyecto_workshop+'/data/trees_pcd')\n",
    "\n",
    "    ruta_actual = os.getcwd()\n",
    "    print(ruta_actual)\n",
    "\n",
    "    SEGMENTOS = lectura_segmentaciones(ruta_actual)\n",
    "    os.chdir(ruta_actual)\n",
    "\n",
    "    numero_de_transformaciones_por_arbol = 3\n",
    "\n",
    "    def rotar_punto(origen, punto, angulo):\n",
    "\n",
    "        # Ángulo en radianes\n",
    "\n",
    "        ox, oy = origen\n",
    "        px, py = punto[0],punto[1]\n",
    "\n",
    "        qx = ox + math.cos(angulo) * (px - ox) - math.sin(angulo) * (py - oy)\n",
    "        qy = oy + math.sin(angulo) * (px - ox) + math.cos(angulo) * (py - oy)\n",
    "        return np.array([qx, qy,punto[2]])\n",
    "\n",
    "    SEGMENTOS_ARTIFICIALES = {}\n",
    "\n",
    "\n",
    "    contador_arboles = 0\n",
    "    for j in range(numero_de_transformaciones_por_arbol):\n",
    "        for i in range(len(SEGMENTOS)):\n",
    "\n",
    "            segmento = SEGMENTOS[i]\n",
    "\n",
    "            # Todos los árboles que vayamos a crear se verán sometidos a, como mínimo, una\n",
    "            # rotación en el plano XY. Para ello calculamos primero su eje de simetría:\n",
    "\n",
    "\n",
    "            cgx = (1/len(segmento.points))*(np.sum(np.array(segmento.points).take(0,1)))\n",
    "            cgy = (1/len(segmento.points))*(np.sum(np.array(segmento.points).take(1,1)))\n",
    "            #cgz = (1/len(segmento.points))*(np.sum(np.array(segmento.points).take(2,1)))\n",
    "\n",
    "            centro_de_masas = [cgx,cgy]\n",
    "\n",
    "            puntos_rotados = []\n",
    "\n",
    "            angulo = np.random.random() # En radianes lo cogemos\n",
    "\n",
    "            for punto in segmento.points:\n",
    "                puntos_rotados.append(rotar_punto(centro_de_masas, punto, angulo))\n",
    "\n",
    "            puntos_rotados = np.array(puntos_rotados)\n",
    "\n",
    "            segmento_rotado = o3d.geometry.PointCloud()\n",
    "            segmento_rotado.points = o3d.utility.Vector3dVector(puntos_rotados)\n",
    "\n",
    "\n",
    "            # Ahora le vamos a meter algo de ruido (pero sólo a los puntos que\n",
    "            # estén por encima de una determinada altura).\n",
    "\n",
    "            dispersion = np.mean(segmento_rotado.compute_point_cloud_distance(segmento))\n",
    "\n",
    "\n",
    "            altura_media = np.mean(puntos_rotados[:,2])\n",
    "\n",
    "            indices = np.where(puntos_rotados[:,2]>altura_media)[0]\n",
    "\n",
    "            ruido_x = np.random.normal(0,dispersion,len(indices))\n",
    "            ruido_y = np.random.normal(0,dispersion,len(indices))\n",
    "            ruido_z = np.random.normal(0,0.5,len(indices))\n",
    "\n",
    "            # print(indices)\n",
    "            # print()\n",
    "            # print(puntos_rotados)\n",
    "\n",
    "            puntos_rotados[indices, 0] = np.reshape(puntos_rotados[indices, 0] + ruido_x, -1)\n",
    "            puntos_rotados[indices, 1] = np.reshape(puntos_rotados[indices, 1] + ruido_y, -1)\n",
    "            puntos_rotados[indices, 2] = np.reshape(puntos_rotados[indices, 2] + ruido_z, -1)\n",
    "\n",
    "\n",
    "            # Volvemos a definir el nuevo árbol:\n",
    "            arbol_artificial = o3d.geometry.PointCloud()\n",
    "            arbol_artificial.points = o3d.utility.Vector3dVector(puntos_rotados)\n",
    "\n",
    "            # Lo pintamos con un color:\n",
    "            lista_colores_arbol = [np.array([0/255,51/255,0/255]),\n",
    "                             np.array([0/255,102/255,0/255]),\n",
    "                             np.array([0/255,153/255,0/255]),\n",
    "                             np.array([0/255,204/255,0/255]),\n",
    "                             np.array([0/255,255/255,0/255]),\n",
    "                             np.array([51/255,255/255,51/255]),\n",
    "                             np.array([102/255,255/255,102/255])]\n",
    "\n",
    "            colores_arbol = np.zeros((len(puntos_rotados),3))\n",
    "            for e in range(len(colores_arbol)):\n",
    "                colores_arbol[e] = lista_colores_arbol[np.random.choice(len(lista_colores_arbol))]\n",
    "\n",
    "            arbol_artificial.colors = o3d.utility.Vector3dVector(colores_arbol)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # El False se lo pongo para designar que no ha sido seleccionado to-\n",
    "            # davía:\n",
    "            SEGMENTOS_ARTIFICIALES[contador_arboles] = [arbol_artificial,False]\n",
    "            contador_arboles += 1\n",
    "\n",
    "\n",
    "\n",
    "    import copy\n",
    "\n",
    "    ARBOLES = []\n",
    "    NUBES_ARTIFICIALES = {}\n",
    "    #superficie = SUPERFICIES[0]\n",
    "    superficie = pcd_DTM\n",
    "    ARBOLES.append([])\n",
    "\n",
    "    Numero_arboles_por_nube = 100\n",
    "\n",
    "    puntos_superficie = np.array(superficie.points)\n",
    "\n",
    "\n",
    "    SEGMENTOS_ARTIFICIALES_aux = copy.deepcopy(SEGMENTOS_ARTIFICIALES)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    contador = 0\n",
    "    cuntudur = 0\n",
    "    for j in range(Numero_arboles_por_nube):\n",
    "\n",
    "        # Si ya hemos terminado de poner todos los árboles que teníamos\n",
    "        # creados volvemos a empezar:\n",
    "        if cuntudur == len(SEGMENTOS_ARTIFICIALES_aux):\n",
    "            # print(SEGMENTOS_ARTIFICIALES[0])\n",
    "            # SEGMENTOS_ARTIFICIALES_aux.clear()\n",
    "            SEGMENTOS_ARTIFICIALES_aux = copy.deepcopy(SEGMENTOS_ARTIFICIALES)\n",
    "            # print(SEGMENTOS_ARTIFICIALES[0])\n",
    "            cuntudur = 0\n",
    "\n",
    "        posible_ubicacion = puntos_superficie[np.random.choice(len(puntos_superficie))]\n",
    "\n",
    "        indice = np.random.choice(len(SEGMENTOS_ARTIFICIALES_aux))\n",
    "        # Cojo un árbol al azar:\n",
    "        arbol = SEGMENTOS_ARTIFICIALES_aux[indice]\n",
    "        # arbol = SEGMENTOS_ARTIFICIALES[10] # Ejemplo\n",
    "\n",
    "        while arbol[1] == True:\n",
    "            indice = np.random.choice(len(SEGMENTOS_ARTIFICIALES_aux))\n",
    "            arbol = SEGMENTOS_ARTIFICIALES_aux[indice]\n",
    "        if arbol[1] == False:\n",
    "            cuntudur += 1\n",
    "\n",
    "        arbol = arbol[0]\n",
    "        # Le ponemosla etiqueta True porque ya ha sido seleccionado:\n",
    "        SEGMENTOS_ARTIFICIALES_aux[indice][1] = True\n",
    "        SEGMENTOS_ARTIFICIALES[indice][1] = False\n",
    "        # if cuntudur == 1:\n",
    "        #     print(SEGMENTOS_ARTIFICIALES_aux[indice][1])\n",
    "        #     print(SEGMENTOS_ARTIFICIALES[indice][1])\n",
    "        arbol.translate((posible_ubicacion),relative=False)\n",
    "\n",
    "        puntos_arbol = np.array(arbol.points)\n",
    "        minima_altura = puntos_arbol.take(2,1).min()\n",
    "\n",
    "        color_arbol = np.array(arbol.colors)\n",
    "\n",
    "        desfase_vertical = posible_ubicacion-minima_altura\n",
    "\n",
    "        puntos_arbol[:, 2] = puntos_arbol.take(2,1) + desfase_vertical[2]\n",
    "        arbol = o3d.geometry.PointCloud()\n",
    "        arbol.points = o3d.utility.Vector3dVector(puntos_arbol)\n",
    "        arbol.colors = o3d.utility.Vector3dVector(color_arbol)\n",
    "\n",
    "\n",
    "        ARBOLES[-1].append(arbol)\n",
    "\n",
    "        if contador == 0:\n",
    "\n",
    "            nube_artificial = arbol+superficie\n",
    "            contador += 1\n",
    "\n",
    "        else:\n",
    "            nube_artificial += arbol\n",
    "\n",
    "\n",
    "    # Metemos todos los árboles que coleccionamos en un sólo elemento:\n",
    "    arbol = o3d.geometry.PointCloud()\n",
    "\n",
    "    for arbol_i in ARBOLES[-1]:\n",
    "        arbol += arbol_i\n",
    "\n",
    "    # Cambiamos tooodos los elementos por uno sólo que serán todos los ár-\n",
    "    # boles:\n",
    "    ARBOLES = arbol\n",
    "    NUBES_ARTIFICIALES = nube_artificial\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    os.chdir(ruta_proyecto_workshop+'/data/synthetic_point_clouds')\n",
    "    try:\n",
    "        os.mkdir('SYNTHETIC_DATASET')\n",
    "    except FileExistsError:\n",
    "        print('The folder already exists, so we pass')\n",
    "    os.chdir('SYNTHETIC_DATASET')\n",
    "\n",
    "    path_dataset = os.getcwd()\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('Nube_artificial_%s'%str(XD))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    os.chdir('Nube_artificial_%s'%str(XD))\n",
    "\n",
    "\n",
    "    os.mkdir('numpy_arrays')\n",
    "    os.chdir('numpy_arrays')\n",
    "    \n",
    "    # Too many creations of subfolders, I know but this is a modified version of a original simulator that I developed... Sorry ;)\n",
    "\n",
    "    puntos_arboles = np.array(ARBOLES.points)\n",
    "    with open(\"arboles.npy\", 'wb') as f:    \n",
    "        np.save(f, puntos_arboles)\n",
    "    with open(\"DTM.npy\", 'wb') as f:    \n",
    "        np.save(f, puntos_superficie) \n",
    "    \n",
    "    puntos_nube = np.vstack((puntos_arboles,puntos_superficie))\n",
    "    with open(\"Nube_artificial_%s.npy\"%str(XD), 'wb') as f:    \n",
    "        np.save(f, puntos_superficie) \n",
    "\n",
    "        \n",
    "    import shutil\n",
    "    os.chdir(path_dataset)\n",
    "    if XD <= 6:\n",
    "        try:\n",
    "            os.mkdir('TRAIN')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        original = os.getcwd()+'/Nube_artificial_%s'%str(XD)\n",
    "        target = os.getcwd()+'/TRAIN/Nube_artificial_%s'%str(XD)\n",
    "        shutil.move(original, target)\n",
    "    elif 6 < XD < 9:\n",
    "        try:\n",
    "            os.mkdir('TEST')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        original = os.getcwd()+'/Nube_artificial_%s'%str(XD)\n",
    "        target = os.getcwd()+'/TEST/Nube_artificial_%s'%str(XD)\n",
    "        shutil.move(original, target)        \n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir('PREDICT')\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        original = os.getcwd()+'/Nube_artificial_%s'%str(XD)\n",
    "        target = os.getcwd()+'/PREDICT/Nube_artificial_%s'%str(XD)\n",
    "        shutil.move(original, target)       \n",
    "    \n",
    "    instante_fin_creacion_nube = time.time()\n",
    "    \n",
    "    print('Nube generada. Transcurrido por ahora: ',(instante_fin_creacion_nube-instante_inicial)/60.,' min')\n",
    "        \n",
    "    os.chdir(ruta_proyecto_workshop)\n",
    "\n",
    "print('FIN GENERACIÓN DATASET')\n",
    "\n",
    "instante_final = time.time()\n",
    "\n",
    "print('Duración TOTAL: ',(instante_final-instante_inicial)/60.,' min')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16aa017",
   "metadata": {},
   "source": [
    "## 3.2. Training and validation of PointNet++\n",
    "\n",
    "Now that we have our synthetic dataset created, it's time to run PointNet++ in an **udocker container**. Before starting, some manually work is required. In the file **train_5_layers.py** [located here](./Deep_Learning/pointnet2/scannet/traduccion_lino/debug/Santarem) we will make the following change according to the paths of your specific machine:\n",
    "\n",
    "<center>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>REPLACE:</b> DATASET_PATH should points to your SYNTHETIC_DATASET path \n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Also, it is required to modify the file named **correr_pointnet_udocker.sh**, which is located in the [PointNet++ directory](./Deep_Learning/pointnet2):\n",
    "\n",
    "<center>\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Line 186:</b> DATASET_PATH should points to your SYNTHETIC_DATASET path \n",
    "</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "Now we are fully ready to start training and validating the DL model.\n",
    "\n",
    "Open a terminal and go to the [PointNet++ directory](./Deep_Learning/pointnet2), then run the shell script by doing:\n",
    "\n",
    "```sh correr_pointnet_udocker.sh```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f9ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
